[
  {
    "objectID": "snow_hydro.html",
    "href": "snow_hydro.html",
    "title": "Snow Hydrology Study for Yellowstone National Park",
    "section": "",
    "text": "Field Activites in the Mountians"
  },
  {
    "objectID": "snow_hydro.html#year-long-snow-hydrology-project",
    "href": "snow_hydro.html#year-long-snow-hydrology-project",
    "title": "Snow Hydrology Study for Yellowstone National Park",
    "section": "Year long Snow Hydrology Project",
    "text": "Year long Snow Hydrology Project\nThis is page presents a year long snow hydrology project from my graduate degree. This class was at Colorado State University and was taught by Dr Steven Fassnacht.\nMy focus was on Yellowstone National Park in Wyoming. The goals of this project where to use data from various stations in the greater Yellowstone area and use the data to compute various components of the snowpack and the parameters involved. Data was pulled from the Iowa state Mesonet, Snotel and USGS"
  },
  {
    "objectID": "Trace.html",
    "href": "Trace.html",
    "title": "Trace Events, Precipitation Undercatch, and Probability of Snow",
    "section": "",
    "text": "Billy Johnson October 4, 2024 Dr Fassnacht Advanced Snow Hydrology\nAssignment 5. Trace Events, Precipitation Under catch, and Probability of Snow\nMethods. To begin analysis on trace precipitation events I began with loading in the ASOS data from Yellowstone National Park (Station P60). When downloading data, I told the mesonet website to count trace events as a T. To help keep data frames organized and clean I created a new data frame that only contained the date, and the precipitation recorded. In this data frame I noticed that this station did not record trace events. Knowing this I was not able to create a comparison between trace events and actual recorded precipitation. I used a cumsum function in R studio to calculate the cumulative precipitation for water year 2023 (Figure 1.) For my second question regarding correction for under catch my analysis started again with creating a new data frame with the variables needed (date, precipitation, temperature, windspeed and relative humidity). The next step in my analysis was to get all the different variables into metric units. To help analysis I used the complete.cases function in R to remove all of the rows with missing observations. To then begin analysis on determining the catch efficiency of the ASOS station I create a column that calculated the catch efficiency for snow (CR_snow = exp(-0.04 * wind_ms ^ 1.75)) and the catch efficiency for Mixed precip (Cr_mix = 1.0104-0.0562 * wind_ms) for each observation. I then created a function to find what observations meet the threshold for snow (T &lt;= 0oC) and for mixed precip. (0oC &lt; T &lt;=3oC) and if they meet those conditions I would apply the CR value to the measured precipitation. I was then able to use the cumsum function that calculated the cumulative precipitation with correction for under catch and without correction (figure 2.) The third questions analysis to determine the cumulative hourly precipitation with different thresholds to determine snow began with a new the same data frame from the previous calculation. The first step in this analysis was to create a column using an if statement that determined if precipitation was snow based on air temperature. I used a threshold of 0o C for air temperature. I then used a similar if function to determine if the precipitation was now but with the dew point temperature. I began this step with creating a column that had calculated the dew point temperature for each observation. I then set my if statement to determine if the dew point temperature was below 0o C then it would be recorded as snow. Finally, the last way to determine if the precipitation was snow, I calculated the snow probability curve with Walden Colorado as my reference. This equation gave me fraction of precipitation that was snow for each observation. I told the if statement to determine when the fraction was positive and then record that as snow. I then used the cumsum function to determine the cumulative precipitation from each different threshold (figure 3.). The final question in my analysis was to determine the average monthly wind speed for each type of precipitation (Snow, Rain and No Precipitation). To begin this analysis, I create a new table for average monthly wind speed. I first create a column, using a function to determine if the temperature was below 0 degrees and if it was currently precipitating then label it as snow. I did the same for rain but above 0 degrees. I finally said if none of it was true then label it as no precip. Finally, I grouped the data by month and by precipitation type and calculated the mean. I then plotted these values on a bar chart (figure 4.)\nResults\nFigure 1. The cumulative hourly precipitation (mm) for Yellowstone National Park ASOS station for water year 2023.\nFigure 2. Cumulative precipitation vs corrected cumulative precipitation for water year 2023 at Yellowstone National Park.\nFigure 3. Cumulative precipitation comparison between three different thresholds to determine if precipitation was snow.\nFigure 4. Average monthly wind speed through the year during different precipitation types. No precipitation, snow and rain were compared at Yellowstone National Park.\nDiscussion When looking at the total cumulative precipitation thought the year at Yellowstone National Park (figure 1) we find that most of the precipitation occurs in the winter months. This makes sense based on the type of storms that is generally seen in this part of the country (orographic lift). While the end of the summer and into fall generally do not see the same amount of precipition events. We can look at how much wind had played a factor into creating under catch (figure 2). Based on this analysis we do see an increase in the cumulative precipitation when it was corrected for wind (figure 2) but is not dramatically increased. This does make sense as a result. The average wind speed in this location is not overly high (figure 4) meaning that we may not have a lot of under catch in this specific ASOS station. The general properties of the station may have an impact on the amount of under catch that is recorded. Another reason that this may not be as dramatic of a difference could be because the station did not record trace events. Finally, this result could have been also due to the air temperature threshold that was used. This leads into the third question in the analysis, comparing methods for determine snow with a threshold. In this part of the analysis, we found that using the snow curve equation had a much higher cumulative perceptional total. This could be due to the nature of the equation that was used as it was the only linear reference. The Walden equation is the correct equation for my station as it shares many general climatic properties. In this analysis we also found that the dew point temperature as a threshold also accumulated more snow (figure 3). This makes sense because it considers more of the properties at that specific time. Finally in the last question of my analysis we looked at the average wind speed for each month based on what type of precipitation was falling (figure 4). This graph shows us that overall, through the year the wind is generally low. There is one major increase in wind during December. This is a weird observation because it is also when the precipitation type was rain. This also is a weird observation for December. I think that this could be caused due to the limited number of observations of rain during this month. If one event with rain occurred and it was windy it would cause the graph to have a much higher average than the rest (figure 4). Overall then snow was occurring in that month the wind was higher during those snow events. We find that there are no rain events in January or February, and this makes sense as it would be too cold for it to rain. We also find that the months of July August and September do not have snow events. This also makes sense when comparing it to other climates with similar spatial characteristics."
  },
  {
    "objectID": "Trace.html#wind-during-precipitation-events",
    "href": "Trace.html#wind-during-precipitation-events",
    "title": "Trace Events, Precipitation Undercatch, and Probability of Snow",
    "section": "Wind During Precipitation Events",
    "text": "Wind During Precipitation Events\nCompare the monthly average wind speed during rainfall events, snowfall events, and when no precipitation is occurring. Use a 0oC air temperature threshold to distinguish between rain and snow.\n\nWind During Rainfall events (0 degree threshold)\nWind during snowfall events\nWind when no precipition is occuring\n\n\nMonthly_average_wind_speed &lt;- ASOS_yellowstone_CR_clean %&gt;% \n  mutate(\n    precipitation_type = case_when(\n      tmp_c &gt; 0 & P_corr &gt; 0 ~ \"Rain\",\n      tmp_c &lt;= 0 & P_corr &gt; 0 ~ \"Snow\",\n      TRUE ~ \"No Precipitation\"\n    ),\n    month = format(valid, \"%Y-%m\")\n  ) %&gt;% \n  group_by(month, precipitation_type) %&gt;% \n  summarize(avg_wind_speed = mean(wind_ms, na.rm = TRUE), .groups = 'drop')"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Resume",
    "section": "",
    "text": "Aquatic Ecology | Cold Water Fisheries | Ecosystems | Water Resources | Watershed Sciences\n\n\n\nI am an emerging aquatic ecosystem science professional known for my independence, attention to detail, and collaborative mindset. I am currently pursuing a Professional Master’s degree in Water Resources at Colorado State University.\nMy goal is to work with a mission-driven organization to address water challenges in the Western U.S. I am a self-motivated communicator with a passion for hydrology, shaped by my upbringing in the Colorado mountains as a skier, angler, and lifelong hockey player.\n\n\n\n\n\nGlobally minded with extensive travel experience\n\nHydrometry field measurements\n\nTechnical proficiency in RStudio and ArcGIS\n\nDean’s List honoree\n\nStrong communication & leadership\n\nWatershed science education\n\nTeam-oriented and eager to learn\n\nActive and enthusiastic attitude\n\n\n\n\n\nProfessional Master of Science, Water Resources\nColorado State University, Fort Collins, CO\nAug 2024 – May 2025\nGPA: 4.00\nBachelor of Science, Ecosystem Science and Sustainability\nColorado State University, Fort Collins, CO\nAug 2020 – May 2024\nGPA: 3.61\n\n\n\n\n\n\nWinters 2022 – 2025\n- Provided ski equipment services during peak seasons\n- Strengthened communication and teamwork in fast-paced environment\n- Contributed to the top-rated ski service team in Vail Valley\n\n\n\nJune 2024\n- Conducted electrofishing surveys on Jim Creek\n- Removed over 200 brook trout to support native Cutthroat trout populations\n- Collected data to inform CPW stocking and restoration strategies\n\n\n\nSummers 2020 – 2024\n- Gained hands-on experience with tools and construction projects\n- Worked on municipal projects with multidisciplinary teams\n- Honed strong work ethic and independence\n\n\n\nMay – August 2022\n- Guided anglers in sustainable fishing techniques\n- Taught river stewardship and environmental ethics\n- Applied aquatic ecology knowledge to client education\n\n\n\nJan 2018 – Mar 2020\n- Promoted environmentally friendly ski wax alternatives (e.g., Purl)\n- Supported rental operations during high-demand periods\n\n\n\n\n\n\n\n\nHydrometry and watershed surveys\n\nElectrofishing (Jim Creek, Spring Creek)\n\nWatershed instrumentation and planning\n\nAtmospheric measurements (Kestrel 3000)\n\n\n\n\n\nCPW Jim Creek Restoration Project (2024)\n\nPoudre River Clean-up (2022, 2023)\n\nEagle County Highway Cleanup (2018)\n\nGardener, Betty Ford Alpine Gardens, Vail\n\n\n\n\n\n\n\nAdvanced Snow Hydrology – Processes and impacts on water resources\n\nWatershed Measurement – Field-based data collection skills\n\nHydrometry & Land Use Hydrology – Water quality and flow analysis\n\nDust-on-Snow Research – Capstone on albedo impacts in the San Juans\n\nWater Resources Planning & Management – Infrastructure and systems\n\nR Programming – Statistical analysis and data modeling\n\nGIS – Spatial analysis and map creation\n\nWater Law – Legal foundations and Western water policy\n\nEntomology Research – Sensitive macroinvertebrate species and water quality\n\nBiology of Fishes – Salmonid ecology and life history\n\nSustaining Rivers in a Changing World – Climate change effects\n\nRiver Restoration Design – Process-based restoration planning\n\nRemote Sensing – Interpreting and applying satellite data\n\nModeling Watershed Hydrology – HEC-HMS hydrologic modeling"
  },
  {
    "objectID": "about.html#professional-summary",
    "href": "about.html#professional-summary",
    "title": "Resume",
    "section": "",
    "text": "I am an emerging aquatic ecosystem science professional known for my independence, attention to detail, and collaborative mindset. I am currently pursuing a Professional Master’s degree in Water Resources at Colorado State University.\nMy goal is to work with a mission-driven organization to address water challenges in the Western U.S. I am a self-motivated communicator with a passion for hydrology, shaped by my upbringing in the Colorado mountains as a skier, angler, and lifelong hockey player."
  },
  {
    "objectID": "about.html#key-skills-attributes",
    "href": "about.html#key-skills-attributes",
    "title": "Resume",
    "section": "",
    "text": "Globally minded with extensive travel experience\n\nHydrometry field measurements\n\nTechnical proficiency in RStudio and ArcGIS\n\nDean’s List honoree\n\nStrong communication & leadership\n\nWatershed science education\n\nTeam-oriented and eager to learn\n\nActive and enthusiastic attitude"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Resume",
    "section": "",
    "text": "Professional Master of Science, Water Resources\nColorado State University, Fort Collins, CO\nAug 2024 – May 2025\nGPA: 4.00\nBachelor of Science, Ecosystem Science and Sustainability\nColorado State University, Fort Collins, CO\nAug 2020 – May 2024\nGPA: 3.61"
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "Resume",
    "section": "",
    "text": "Winters 2022 – 2025\n- Provided ski equipment services during peak seasons\n- Strengthened communication and teamwork in fast-paced environment\n- Contributed to the top-rated ski service team in Vail Valley\n\n\n\nJune 2024\n- Conducted electrofishing surveys on Jim Creek\n- Removed over 200 brook trout to support native Cutthroat trout populations\n- Collected data to inform CPW stocking and restoration strategies\n\n\n\nSummers 2020 – 2024\n- Gained hands-on experience with tools and construction projects\n- Worked on municipal projects with multidisciplinary teams\n- Honed strong work ethic and independence\n\n\n\nMay – August 2022\n- Guided anglers in sustainable fishing techniques\n- Taught river stewardship and environmental ethics\n- Applied aquatic ecology knowledge to client education\n\n\n\nJan 2018 – Mar 2020\n- Promoted environmentally friendly ski wax alternatives (e.g., Purl)\n- Supported rental operations during high-demand periods"
  },
  {
    "objectID": "about.html#field-volunteer-experience-20212025",
    "href": "about.html#field-volunteer-experience-20212025",
    "title": "Resume",
    "section": "",
    "text": "Hydrometry and watershed surveys\n\nElectrofishing (Jim Creek, Spring Creek)\n\nWatershed instrumentation and planning\n\nAtmospheric measurements (Kestrel 3000)\n\n\n\n\n\nCPW Jim Creek Restoration Project (2024)\n\nPoudre River Clean-up (2022, 2023)\n\nEagle County Highway Cleanup (2018)\n\nGardener, Betty Ford Alpine Gardens, Vail"
  },
  {
    "objectID": "about.html#relevant-coursework-colorado-state-university",
    "href": "about.html#relevant-coursework-colorado-state-university",
    "title": "Resume",
    "section": "",
    "text": "Advanced Snow Hydrology – Processes and impacts on water resources\n\nWatershed Measurement – Field-based data collection skills\n\nHydrometry & Land Use Hydrology – Water quality and flow analysis\n\nDust-on-Snow Research – Capstone on albedo impacts in the San Juans\n\nWater Resources Planning & Management – Infrastructure and systems\n\nR Programming – Statistical analysis and data modeling\n\nGIS – Spatial analysis and map creation\n\nWater Law – Legal foundations and Western water policy\n\nEntomology Research – Sensitive macroinvertebrate species and water quality\n\nBiology of Fishes – Salmonid ecology and life history\n\nSustaining Rivers in a Changing World – Climate change effects\n\nRiver Restoration Design – Process-based restoration planning\n\nRemote Sensing – Interpreting and applying satellite data\n\nModeling Watershed Hydrology – HEC-HMS hydrologic modeling"
  },
  {
    "objectID": "Projects.html",
    "href": "Projects.html",
    "title": "ESS 523c: Projects",
    "section": "",
    "text": "In this Lab we practiced data wrangling and visualization skills using COVID-19 data that was curated by the New York Times. In this lab we used tidyverse, flextable, and zoo."
  },
  {
    "objectID": "Projects.html#lab-01-covid-19-trends",
    "href": "Projects.html#lab-01-covid-19-trends",
    "title": "ESS 523c: Projects",
    "section": "",
    "text": "In this Lab we practiced data wrangling and visualization skills using COVID-19 data that was curated by the New York Times. In this lab we used tidyverse, flextable, and zoo."
  },
  {
    "objectID": "Projects.html#lab-02-working-with-vector-data---border-summaries",
    "href": "Projects.html#lab-02-working-with-vector-data---border-summaries",
    "title": "ESS 523c: Projects",
    "section": "Lab 02: Working with Vector Data - Border Summaries",
    "text": "Lab 02: Working with Vector Data - Border Summaries\nThis lab focused on exploring the properties of sf, sfc, sfg features and objects. In this lab we used new packages of sf, units, USAboundaries, rnaturalearth, gghighlight, ggrepel, and knitr. Though this lab we calculated the distances between features. We employed projection skills to compare and analyse distances from borders in the United States."
  },
  {
    "objectID": "Projects.html#lab-03-dams-in-the-united-states",
    "href": "Projects.html#lab-03-dams-in-the-united-states",
    "title": "ESS 523c: Projects",
    "section": "Lab 03: Dams In the United States",
    "text": "Lab 03: Dams In the United States\nIn this lab we explored the impacts of tessellated surfaces and the modifiable areal unit problem (MAUP) using the National Dam Inventory. This is maintained by the US Army Corps of Engineers. In this lab we wrote functions, to visualize the distribution of dams and there purposes across the county."
  },
  {
    "objectID": "Projects.html#lab-04-rasters-remote-sensing---palo-iowa-flooding",
    "href": "Projects.html#lab-04-rasters-remote-sensing---palo-iowa-flooding",
    "title": "ESS 523c: Projects",
    "section": "Lab 04: Rasters & Remote Sensing - Palo Iowa Flooding",
    "text": "Lab 04: Rasters & Remote Sensing - Palo Iowa Flooding\nThe focus of Lab 4 was to explore the impacts of flooding in Palo Iowa. This lab explored a real event that occurred on September 26, 2016, when rivers in the area surged producing a flood that recorded water levels 8 feet above flood stage. In the analysis we used terra and rstac packages to create flooding images using multi-band Landsat Imagery, thresholding and classification methods."
  },
  {
    "objectID": "redistribution.html",
    "href": "redistribution.html",
    "title": "Redistribution",
    "section": "",
    "text": "Billy Johnson\nDr Fassnacht\nNovember 2024\nWR 574\nRedistribution\nMethods\n            To begin analysis on the registration at Yellowstone National Park ASOS station, I first used the Li and Pomeroy 1997a equation (eq 1) to calculate the U10 threshold windspeed that was directly related to temperature. The next step was to create a statement that asked if the windspeed was above the U10 threshold and then to state the snow was blowing. After this, I ran a count function to determine the number of hours that snow was blowing, and this was summarized with a bar plot (figure 1).\n            The next step in this project was to determine how much snow was blown around. I used the Pomeroy et al. 1991 equation (eq 2) that calculates the total snow transported. We had to multiply this value by 3.6 due to the conversion factor to get the results into meters/hr. With this quantity, I was able to calculate the cumulative sum of blowing snow and plot this over the year (Figures 2 & 3).\nEquations\n1.     Uthreshold = 9.43 + 0.18 Ta = 0.0033 Ta2\n2.     Qt = 0.0000022 U104.04  \nResults\n            Yellowstone National Park accounted for six total hours of blowing snow based on the U10 threshold. This threshold averaged 9.2 m/s throughout the year. The average wind speed throughout the year at 2 meters, which was recorded from the ASOS station, was only 0.96 m/s. This site doesn’t receive much wind, resulting in only 6 hours of strong enough wind to move snow (figure 1).\nFigure 1. Bar plot showing the number of hours that snow was considered blowing based on the U10 threshold from Li and Pomeroy 1997a.\n            Using those 6 hours of blowing snow, we calculated the total amount of transported snow through distribution as a function of wind speed. This resulted in only 0.024 m/hr of snow being blown (figure 2).\nFigure 2. Bar plot showing the total quantity of snow that was redistributed using the Pomeroy et al. 1991 equation (eq 2)\nFigure 3. Line plot showing the total cumulative snow throughout the year in Yellowstone National Park based on the Pomeroy et al. 1991 equation (eq 2).\nDiscussion\n            The result that came from this analysis showed that only 5 hours of data had strong enough wind to move snow. This seems very low, but when looking at the data that was recorded at the ASOS station, we can see that the average is very low, only averaging 0.96 m/s of wind speed. When the U10 threshold averaged a value close to 10 m/s, we can assume that this event did not occur very many times.\n            The total amount of snow that was transported due to these wind events was also very low. With the correction of m/hr, we only see a total of 0.024 m/hr of snow moved. When we convert this back into something that is more relatable, we have 2.4 mm/hr of snow blowing. This value is still small, but with the average wind speed being so low, we don’t expect much snow to be redistributed. This station doesn’t appear to be a windy location so perhaps this is the total quantity of snow transported. I find this unlikely, but the features around this ASOS station are unknown and may have an impact on the recording of windspeed at this site.\n\nCode\n\nLoad in the Packages\n\nsource(\"Setup_WR574 copy.R\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nterra 1.7.65\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nelevatr v0.99.0 NOTE: Version 0.99.0 of 'elevatr' uses 'sf' and 'terra'.  Use \nof the 'sp', 'raster', and underlying 'rgdal' packages by 'elevatr' is being \ndeprecated; however, get_elev_raster continues to return a RasterLayer.  This \nwill be dropped in future versions, so please plan accordingly.\n\n\n\n\nLoad in the data\n\n# Soda Butte Cr at Park Boundary at Silver Gate (06187915)\nsitenumber &lt;- \"06187915\"\nparametercd &lt;- c(\"00060\", \"00010\")\nstartdate &lt;- \"2023-09-01\"\nenddate &lt;- \"2024-08-31\"\n\nrawUSGSdata &lt;- readNWISdv(sitenumber, parametercd, startdate, enddate)\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06187915&format=waterml%2C1.1&ParameterCd=00060%2C00010&StatCd=00003&startDT=2023-09-01&endDT=2024-08-31\n\n\nASOS data PNA\n\n# ASOS RAMOS Yellowstone\nASOS_yellowstone_original &lt;- read_csv(\"DataIn/P60.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7370 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (24): station, drct, sknt, mslp, vsby, gust, skyc1, skyc2, skyc3, skyc4...\ndbl   (8): lon, lat, elevation, tmpf, dwpf, relh, p01i, alti\ndttm  (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nASOS_clean &lt;- read_csv(\"DataOut/ASOS_clean.csv\")\n\nRows: 246 Columns: 9\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (8): tmpf, p01i, relh, sknt, tmp_c, Td, P_mm, cumm_prec\ndttm (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nASOS_clean_2 &lt;- read_csv(\"DataOut/ASOS_clean_2.csv\")\n\nRows: 7370 Columns: 28\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (13): station, drct, sknt, mslp, vsby, gust, ice_accretion_1hr, ice_acc...\ndbl  (14): tmpf, dwpf, relh, p01i, alti, snowdepth, tmp_c, P_mm, meltrate, H...\ndttm  (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nASOS_accumulation_fresh &lt;- read_csv(\"DataOut/ASOS_accumulation_fresh.csv\")\n\nRows: 246 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr   (1): month\ndbl  (11): tmpf, p01i, relh, sknt, tmp_c, Td, P_mm, cumm_prec, HP_fresh_snow...\ndttm  (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSNOTEL data location: Parker Peak (683)\n\nSNOTEL_Parker &lt;- read_csv(\"DataIn/683_STAND_WATERYEAR=2024_clean.csv\")\n\nRows: 341 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (8): Site Id, WTEQ.I-1 (in), PREC.I-1 (in), TOBS.I-1 (degC), TMAX.D-1 (d...\nlgl (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nSNOTEL_Parker_clean &lt;- read_csv(\"DataOut/Snotel_clean.csv\")\n\nRows: 341 Columns: 13\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\ndbl  (11): Site Id, WTEQ.I-1 (in), PREC.I-1 (in), TOBS.I-1 (degC), TMAX.D-1 ...\nlgl   (1): Time\ndate  (1): Date\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\nMethods.\nRedistribution: Compute the occurrence and quantity of blowing snow:\n\nUsing a threshold wind speed based solely on temperature, determine the monthly frequency of blowing snow. Plot the monthly bar chart of the occurrence of redistribution.\n\n\n\n\nQuestion 1\nMonthly Frequency of blowing snow # You need to use the threshold sand say if the tempature is at this then see if the U10 is above that threshold.\n\nASOS_clean_new &lt;- ASOS_clean_2\n\nASOS_clean_new &lt;- ASOS_clean_new %&gt;% \n  mutate(sknt = as.numeric(sknt)) %&gt;% \n  mutate(wind_speed_ms = sknt/1.944) %&gt;%\n  mutate(U10 = wind_speed_ms * (log(10/0.01) / log(2/0.01))) %&gt;% \n  mutate(U10_thres = 9.43 + (0.18 * (tmp_c)) + (0.0033 * (tmp_c * exp (2))))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `sknt = as.numeric(sknt)`.\nCaused by warning:\n! NAs introduced by coercion\n\nASOS_clean_new &lt;- ASOS_clean_new %&gt;% \n  mutate(blowing = U10 &gt; U10_thres)\n\nASOS_clean_new %&gt;%\n  mutate(month = month(valid)) %&gt;%  \n  group_by(month) %&gt;%\n  summarise(cum_blowing = sum(blowing, na.rm = TRUE)) %&gt;% \n  ggplot(aes(x = factor(month), y = cum_blowing)) +\n  geom_col() +\n  theme_linedraw()+\n  labs(\n    x = \"Month\",\n    y = \"Hours of Blowing Snow\"\n  )\n\n\n\n\n\n\n\n\n\nUsing the Pomeroy et al. (1991) total transport equation, estimate the cumulative hourly total quantity of blowing snow. Note that snow is only moved when the threshold wind speed is met or exceeded. Plot the cumulative hourly redistribution loss.\n\n\nASOS_clean_new &lt;- ASOS_clean_new %&gt;% \n  mutate(TTE = ifelse(blowing == \"TRUE\", (U10 * exp(4.2)) * 0.0000022 * 3.6 , NA)) \n\nASOS_clean_new %&gt;% \n  mutate(month = month(valid)) %&gt;% \n  group_by(month) %&gt;% \n  summarise(cum_total = sum(TTE, na.rm = TRUE)) %&gt;% \n  ggplot(aes(x = factor(month), y = cum_total))+\n  geom_col()+\n  theme_linedraw()+\n  labs(\n    x = \"Month\",\n    y = \"Total Snow Redistributed (m/hr)\"\n  )\n\n\n\n\n\n\n\nASOS_clean_new %&gt;% \n  mutate(month = month(valid)) %&gt;% \n  mutate(cum_sum_total = cumsum(TTE)) %&gt;% \n  ggplot(aes(x = factor(month), y = cum_sum_total))+\n  geom_line()+\n  theme_linedraw()\n\nWarning: Removed 7370 rows containing missing values or values outside the scale range\n(`geom_line()`).\n\n\n\n\n\n\n\n\nASOS_clean_new %&gt;% \n  filter(!is.na(TTE)) %&gt;%\n  mutate(month = month(valid)) %&gt;% \n  arrange(valid) %&gt;% \n  mutate(cum_sum_total = cumsum(TTE)) %&gt;%  \n  ggplot(aes(x = valid, y = cum_sum_total)) +  \n  geom_line() +\n  theme_linedraw() +\n  labs(\n    x = \"Date\",\n    y = \"Cumulative Total of Blown Snow Redistribution (m/hr)\"\n  )\n\n\n\n\n\n\n\n\n\nwrite_csv(ASOS_clean_new, file = \"DataOut/redistribution_assignemnt.csv\")"
  },
  {
    "objectID": "Accumulation.html",
    "href": "Accumulation.html",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "",
    "text": "Billy Johnson Dr Fassnacht October 2024 WR 574 Snow Hydrology\nAssignment 6: Accumulation, Fresh Snow and Albedo\nMethods\ns(t) = [ s(t-1) - s-min] e-kt + s-min (equation 2)\nResults\nFigure 1. Bar plot showing snow densities in kg/m3 for Yellowstone national park ASOS data.\nFigure 2. Cumulative snowfall shown as a line graph for Yellowstone National Park ASOS station.\nSnow Depth: After analysis of our SNOTEL data, we were able to compute the cumulative snow depth to be 5156.2 mm (figure 3). The highest observed snow depth at the SNOTEL station was measured on March 3, 2024, at 2006.6 mm.\nFigure 3. Time series plot showing both observed snow depth at the SNOTEL station (693) as well as computed cumulative depth.\nAlbedo: After visual analysis of the albedo throughout the year most observations with 0.15 % albedo occurred in the summer when there was no snow to reflect light. During the winter months or time when snow was accumulating on the ground the albedo was consistently high. Snow depth from the ASOS station at Yellowstone RAMOS had a max height of 3600 mm. Snow had disappeared by July 2024 (figure 4).\nFigure 4. Measured snow depth at ASOS station RAMOS in Yellowstone National Park and calculated albedo for water year 2023-2024.\nFresh Snow Density: After calculating the fresh snow density using the relationship between snow depth and SWE, we can see the relationship between the average temperature and the density of fresh snow. The relationship not significant but does show relationships.\nFigure 5. Point plot showing the relationship between the average daily air temperature and fresh snow density at Yellowstone National Park.\nDiscussion:\nThrough the analysis in the fresh snow problem one thing that came to mind was the insignificant variance between months. I assume that the months have quite different average temperatures and receive different amounts of snow, all would lead to a variance between months and the average density. In this analysis I also found that June had a very high average snow density. This is caused from one event and this high density relates to high temperatures when this event would have occurred. The cumulative snowfall made a lot of sense. Most of the snowfall occurred in the late winter which is relatable to the rest of mountainous regions in the Rocky Mountains.\nSnow depth in Yellowstone National Park was as one would expect. The peak of the snow depth was in March. This is a reasonable time.\nThrough the albedo analysis these results seemed to line up well with the measured snow depth. The storm events that occur in October match directly with changes in Albedo. When the snow begins to melt at the end of the winter the albedo levels on average decrease. During the winter months when much of the area is covered with snow, and we have a snow depth of over 3000 mm then the average albedo % is around 0.8. There is a lot of noise at the peak snow depth because of all the flux in albedo as the snowpack starts to shirt into a melting phase.\nIn the last problem the hypothesis for me would have been that as temperatures got colder than the density of fresh snow would decrease. In figure 5 we have a slight inclination that as temperatures increase then the density of snow also increases thus confirming our hypothesis. This relationship is not strongly correlated but we do get a visual representation of the overall trend in data."
  },
  {
    "objectID": "Accumulation.html#accumulation",
    "href": "Accumulation.html#accumulation",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "1. Accumulation:",
    "text": "1. Accumulation:\nAssuming that ground temperature is equal to air temperature prior to snowfall and using an appropriate threshold (e.g., 0oC dewpoint temperature) for snow to fall near the ground (state which temperature threshold you use) to initiate accumulation, determine when complete snow cover occurs (state all assumptions). What day and time is there complete snow cover at your ASOS station? (No plot, just a date and time.)\nThreshold (air and dew) = 0 degrees celcius\n\n# Filter for the relevant columns\nASOS_accumulation &lt;- ASOS_yellowstone_original %&gt;% \n  select(c(valid, tmpf, p01i, relh, sknt)) %&gt;% \n  mutate(tmp_c = (5/9 * (tmpf - 32))) %&gt;% \n  mutate(Td = tmp_c - ((100 - relh)/5))\n  \n# Filter for only events that are below 0 degrees\nASOS_accumulation_clean &lt;- ASOS_accumulation %&gt;% \n  filter(tmp_c &lt; 0 & Td &lt; 0) %&gt;% \n  filter(p01i &gt; 0) %&gt;% \n  filter(sknt &lt; 10)\n\n# Add a column that calculates the cumulative sum\nASOS_accumulation_clean &lt;- ASOS_accumulation_clean %&gt;% \n  mutate(P_mm = p01i * 24.5) %&gt;% \n  mutate(cumm_prec = cumsum(P_mm)) \n\n27th of October\nwhat constitutes complete snow cover??? =\nHow realistic is this (for discussion)"
  },
  {
    "objectID": "Accumulation.html#fresh-snow",
    "href": "Accumulation.html#fresh-snow",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "2. Fresh snow:",
    "text": "2. Fresh snow:\n\nEstimate the hourly density of the fresh snow added to the snowpack (i.e., when it is snowing) using the Hedstrom-Pomeroy or another relevant equation. Plot either the average monthly fresh snow density or the hourly fresh snow density; and\n\nHedstrom-Pomeroy equation = Ps-fresh = 67.92 + 51.25 e ^(Ta/2.29)\n\nestimate and plot the net (cumulative) hourly snow depth without metamorphism, also known as snowfall.\n\n\n# for Fresh Snow Density Calibration, you \"applied the Hedstrom and Pomeroy to the data ...\" - you should be using the d_SWE/d_ds when d_ds &gt; 0 to compute density\n\n# Filter for when its snowing and add the fresh snow density to the dataframe\nASOS_accumulation_fresh &lt;- ASOS_accumulation_clean %&gt;% \n  filter(P_mm &gt; 0 ) %&gt;% \n  mutate(HP_fresh_snow = 67.92 + 51.25 * exp(tmp_c / 2.59))\n\n# find snowfall depth with relationship between precip and density and calculate the cummualtive snowfall\nASOS_accumulation_fresh &lt;- ASOS_accumulation_fresh %&gt;% \n  mutate(snowfall = (P_mm /HP_fresh_snow) * 1000) %&gt;% \n  mutate(cummulative_snowfall = cumsum(snowfall))\n\nASOS_accumulation_fresh &lt;- ASOS_accumulation_fresh %&gt;% \n  mutate(month = month(valid, label = TRUE))\n\nASOS_accumulation_fresh %&gt;%\n  group_by(month) %&gt;%\n  summarise(avg_density = mean(HP_fresh_snow, na.rm = TRUE)) %&gt;%\n  ggplot(aes(x = month, y = avg_density)) +\n  geom_bar(stat = \"identity\", fill = \"lightblue3\")+\n  labs( \n       x = \"Month\", \n       y = \"Snow Density (kg/m³)\")+\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\n# Plot for cummulative snowfall\nASOS_accumulation_fresh %&gt;% \n  ggplot(aes(x = valid, y = cummulative_snowfall))+\n  geom_line()+\n  labs(\n    x = \"Date\",\n    y = \"Cumulative Snowfall (mm)\"\n  )+\n  theme_linedraw()\n\n\n\n\n\n\n\n\n\nwrite_csv(ASOS_accumulation_fresh, \"DataOut/ASOS_accumulation_fresh.csv\")"
  },
  {
    "objectID": "Accumulation.html#snow-depth-from-the-daily-snotel-snow-depth-data-compute-the-cumulative-snowfall-i.e.-the-sum-of-all-positive-snow-depth-amounts.-on-the-same-graph-compare-the-computed-snowfall-to-the-amount-of-snow-measured-on-the-ground-observed-snow-depth.",
    "href": "Accumulation.html#snow-depth-from-the-daily-snotel-snow-depth-data-compute-the-cumulative-snowfall-i.e.-the-sum-of-all-positive-snow-depth-amounts.-on-the-same-graph-compare-the-computed-snowfall-to-the-amount-of-snow-measured-on-the-ground-observed-snow-depth.",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "3. Snow Depth: From the daily SNOTEL snow depth data, compute the cumulative “snowfall,” i.e., the sum of all positive snow depth amounts. On the same graph, compare the computed snowfall to the amount of snow measured on the ground (observed snow depth).",
    "text": "3. Snow Depth: From the daily SNOTEL snow depth data, compute the cumulative “snowfall,” i.e., the sum of all positive snow depth amounts. On the same graph, compare the computed snowfall to the amount of snow measured on the ground (observed snow depth).\n\nSNOTEL_Parker_clean &lt;- SNOTEL_Parker %&gt;% \n  mutate(`SNWD.I-1 (in)` = ifelse(`SNWD.I-1 (in)` &lt; 0, 0, `SNWD.I-1 (in)`)) %&gt;% \n  mutate(snow_depth_mm = `SNWD.I-1 (in)` * 25.4) %&gt;% \n  mutate(delta_snowdepth = snow_depth_mm - lag(snow_depth_mm, 1))\n\nSNOTEL_Parker_clean &lt;- SNOTEL_Parker_clean %&gt;% \n  mutate(delta_snowdepth = ifelse(delta_snowdepth &lt; 0, 0, delta_snowdepth)) %&gt;% \n  mutate(delta_snowdepth = replace_na(delta_snowdepth, 0)) %&gt;% \n  mutate(cumm_snowdepth = cumsum(delta_snowdepth))\n  \nSNOTEL_Parker_clean &lt;- SNOTEL_Parker_clean %&gt;% \n  mutate(Date = mdy(Date))\n\n\nSNOTEL_Parker_clean %&gt;%\n  ggplot(aes(x = Date)) +\n  geom_line(aes(y = cumm_snowdepth, color = \"Cumulative Snowfall\")) +\n  geom_line(aes(y = snow_depth_mm, color = \"Snow Depth (mm)\")) +\n  labs(\n       x = \"Date\",\n       y = \"Depth (mm)\",\n       color = \"Legend\") +  # Add a label for the legend\n  scale_color_manual(values = c(\"Cumulative Snowfall\" = \"blue\", \n                                \"Snow Depth (mm)\" = \"red\"))+\n  theme_linedraw()+\n  theme(legend.position = c(0.2, 0.8),  # Position the legend inside the plot\n        legend.background = element_rect(fill = \"transparent\"))\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\n\n\n\n\n\n\nwrite_csv(SNOTEL_Parker_clean, file = \"DataOut/Snotel_clean.csv\")"
  },
  {
    "objectID": "Accumulation.html#albedo-estimate-and-plot-the-albedo-on-an-hourly-basis.-state-the-assumption-for-the-albedo-for-soil-when-there-is-no-snow-accumulated.-use-a-fresh-snow-albedo-of-0.84-and-the-first-order-exponential-function-to-model-albedo.-after-a-snowfall-without-melting-allow-albedo-to-decay-to-0.70-and-to-0.50-during-melt-i.e.-t-0oc.",
    "href": "Accumulation.html#albedo-estimate-and-plot-the-albedo-on-an-hourly-basis.-state-the-assumption-for-the-albedo-for-soil-when-there-is-no-snow-accumulated.-use-a-fresh-snow-albedo-of-0.84-and-the-first-order-exponential-function-to-model-albedo.-after-a-snowfall-without-melting-allow-albedo-to-decay-to-0.70-and-to-0.50-during-melt-i.e.-t-0oc.",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "4. Albedo: Estimate and plot the albedo on an hourly basis. State the assumption for the albedo for soil when there is no snow accumulated. Use a fresh snow albedo of 0.84, and the first order exponential function to model albedo. After a snowfall without melting allow albedo to decay to 0.70 and to 0.50 during melt, i.e., T > 0oC.",
    "text": "4. Albedo: Estimate and plot the albedo on an hourly basis. State the assumption for the albedo for soil when there is no snow accumulated. Use a fresh snow albedo of 0.84, and the first order exponential function to model albedo. After a snowfall without melting allow albedo to decay to 0.70 and to 0.50 during melt, i.e., T &gt; 0oC.\nfresh_snow_albedo = 0.84 snowfall_without_melt = 0.70 snowfall_with_melt = 0.50\n\nASOS_yellowstone_clean &lt;- ASOS_yellowstone_original %&gt;% \n  select(!c(lon, lat, metar, elevation, skyc1, skyc2, skyc3, skyc4, skyl1:skyl4, wxcodes)) %&gt;% \n  mutate(tmp_c = 5/9 * (tmpf - 32)) %&gt;%\n  mutate(P_mm = p01i * 25.4)\n\nASOS_yellowstone_clean_2 &lt;- ASOS_yellowstone_clean %&gt;% \n  mutate(meltrate = ifelse(tmp_c &gt; 0, tmp_c * 0.5, 0)) %&gt;% \n  mutate(HP_fresh_snow = 67.92 + 51.25 * exp(tmp_c / 2.59)) %&gt;% \n  mutate(snowfall = (P_mm /HP_fresh_snow) * 1000) %&gt;% \n  mutate(cummulative_snowfall = cumsum(snowfall))\n\nthe first order exponential function –&gt; y = Ae^Bx From the ASOS station, we are calculating it for something down the road.\n\n# Create vectors from the columns\nmelts &lt;- ASOS_yellowstone_clean_2$meltrate\nfalls &lt;- ASOS_yellowstone_clean_2$snowfall\n\n# Initialize an empty vector for snow depth and snowdepth counter\ndepths &lt;- numeric(length(falls))  # Pre-allocate space for efficiency\nsnowdepth &lt;- 0\n\n# Loop through the falls vector and apply the logic\nfor (i in seq_along(falls)) {\n  snowdepth &lt;- snowdepth + falls[i]  # Add snowfall\n  \n  # Apply the melt if snowdepth is greater than 0\n  if (snowdepth &gt; 0) {\n    melt &lt;- melts[i]\n  } else {\n    melt &lt;- 0\n  }\n  \n  snowdepth &lt;- snowdepth - melt  # Subtract the melt from snowdepth\n  \n  # Store the result in the depths vector\n  depths[i] &lt;- snowdepth\n}\n\n# Add the calculated depths back to the dataframe if needed\nASOS_yellowstone_clean_2$snowdepth &lt;- depths\n\nASOS_yellowstone_clean_2 &lt;- ASOS_yellowstone_clean_2 %&gt;% \n  mutate(delta_snow_depth = snowdepth - lag(snowdepth, 1))\n\n\nASOS_yellowstone_clean_2 %&gt;% \n  ggplot(aes(x = valid, y = snowdepth))+\n  geom_line()+\n  theme_linedraw()+\n  labs(\n    x = \"Date\",\n    y = \"Snow Depth (mm)\"\n  )\n\n\n\n\n\n\n\n\n\nsoil_albedo &lt;- 0.15\nfresh_albedo &lt;- 0.84\nfreezing_min_albedo &lt;- 0.70\nmelting_min_albedo &lt;- 0.50\nk &lt;- 0.01\n\n\ncalculate_albedo &lt;- function(prev_albedo, snowdepth, snow_depth_change) {\n  if (snowdepth &lt;= 0) {\n    return(soil_albedo)\n  } else if (snow_depth_change &gt; 0) {\n    return(fresh_albedo)\n  } else if (snow_depth_change == 0) {\n    return(pmax(freezing_min_albedo, prev_albedo * exp(-k) + freezing_min_albedo * (1 - exp(-k))))\n  } else if (snow_depth_change &lt; 0) {\n    return(pmax(melting_min_albedo, prev_albedo * exp(-k) + melting_min_albedo * (1 - exp(-k))))\n  } else {\n    return(NA_real_)\n  }\n}\n\nASOS_yellowstone_clean_2 &lt;- ASOS_yellowstone_clean_2 %&gt;%\n  arrange(valid) %&gt;%  # Ensure the data is in chronological order\n  mutate(\n    prev_snowdepth = lag(snowdepth, default = first(snowdepth)),\n    snow_depth_change = snowdepth - prev_snowdepth\n  ) %&gt;%\n  mutate(\n    albedo = accumulate2(\n      .x = snowdepth,\n      .y = snow_depth_change,\n      .f = calculate_albedo,\n      .init = soil_albedo\n    )[-1]  # Remove the initial value\n  ) %&gt;%\n  select(-prev_snowdepth, -snow_depth_change)  # Remove temporary columns\n\n# Print the first few rows to check the result\nprint(head(ASOS_yellowstone_clean_2))\n\n# A tibble: 6 × 28\n  station valid                tmpf  dwpf  relh drct   sknt   p01i  alti mslp   \n  &lt;chr&gt;   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt;  \n1 P60     2023-09-01 00:56:00    46    34  62.7 null   3.00      0  30.2 1015.20\n2 P60     2023-09-01 01:56:00    46    37  70.6 0.00   0.00      0  30.2 1014.30\n3 P60     2023-09-01 02:56:00    42    36  79.1 310.00 3.00      0  30.1 1013.80\n4 P60     2023-09-01 03:56:00    40    35  82.2 0.00   0.00      0  30.1 1013.40\n5 P60     2023-09-01 04:56:00    39    35  85.4 310.00 3.00      0  30.1 1013.30\n6 P60     2023-09-01 05:56:00    38    36  92.4 290.00 3.00      0  30.1 1013.50\n# ℹ 18 more variables: vsby &lt;chr&gt;, gust &lt;chr&gt;, ice_accretion_1hr &lt;chr&gt;,\n#   ice_accretion_3hr &lt;chr&gt;, ice_accretion_6hr &lt;chr&gt;, peak_wind_gust &lt;chr&gt;,\n#   peak_wind_drct &lt;chr&gt;, peak_wind_time &lt;chr&gt;, feel &lt;chr&gt;, snowdepth &lt;dbl&gt;,\n#   tmp_c &lt;dbl&gt;, P_mm &lt;dbl&gt;, meltrate &lt;dbl&gt;, HP_fresh_snow &lt;dbl&gt;,\n#   snowfall &lt;dbl&gt;, cummulative_snowfall &lt;dbl&gt;, delta_snow_depth &lt;dbl&gt;,\n#   albedo &lt;dbl&gt;\n\n\ngraph\n\nASOS_yellowstone_clean_2 %&gt;% \n  ggplot()+\n  geom_line(aes(x = valid, y = albedo))+\n  theme_linedraw()+\n  labs(\n    x = \"Date\",\n    y = \"Albedo\"\n  )\n\n\n\n\n\n\n\nASOS_yellowstone_clean_2 %&gt;% \n  ggplot(aes(x = valid)) +\n  geom_line(aes(y = albedo, color = \"Albedo\")) +\n  geom_line(aes(y = snowdepth / max(snowdepth, na.rm = TRUE) * max(albedo, na.rm = TRUE), color = \"Snow Depth\")) +\n  scale_y_continuous(\n    name = \"Albedo\",\n    sec.axis = sec_axis(~ . * max(ASOS_yellowstone_clean_2$snowdepth, na.rm = TRUE) / max(ASOS_yellowstone_clean_2$albedo, na.rm = TRUE), \n                        name = \"Snow Depth (mm)\")\n  ) +\n  scale_color_manual(values = c(\"Albedo\" = \"#FFA07A\", \"Snow Depth\" = \"blue\")) +\n  theme_linedraw() +\n  labs(\n    x = \"Date\",\n    color = \"Variable\"\n  ) +\n  theme(legend.position = \"bottom\")\n\n\n\n\n\n\n\nwrite_csv(ASOS_yellowstone_clean_2, file = \"DataOut/ASOS_clean_2.csv\")"
  },
  {
    "objectID": "Accumulation.html#fresh-snow-density-calibration-use-your-daily-snotel-swe-snow-depth-and-temperature-data-to-develop-a-relation-between-fresh-snow-density-and-average-air-temperature.-state-your-assumptions-and-the-reliability-of-the-relation.",
    "href": "Accumulation.html#fresh-snow-density-calibration-use-your-daily-snotel-swe-snow-depth-and-temperature-data-to-develop-a-relation-between-fresh-snow-density-and-average-air-temperature.-state-your-assumptions-and-the-reliability-of-the-relation.",
    "title": "Accumulation, Fresh Snow and Albedo",
    "section": "5. Fresh Snow Density Calibration: Use your daily SNOTEL SWE, snow depth, and temperature data to develop a relation between fresh snow density and average air temperature. State your assumptions and the reliability of the relation.",
    "text": "5. Fresh Snow Density Calibration: Use your daily SNOTEL SWE, snow depth, and temperature data to develop a relation between fresh snow density and average air temperature. State your assumptions and the reliability of the relation.\n\nFresh_snow_calibration &lt;- SNOTEL_Parker_clean %&gt;%  \n  mutate( SWE = `WTEQ.I-1 (in)`) %&gt;% \n  mutate(density_fresh_snow = SWE/snow_depth_mm) \n\n\nFresh_snow_calibration %&gt;% \n  ggplot(aes(x = density_fresh_snow, y = `TAVG.D-1 (degC)`))+\n  geom_point()+\n  theme_linedraw()+\n  labs(\n    y = \"Average Temperature (Celsius) \",\n    x = \"Desnity of Fresh Snow (kg/m3)\"\n  )\n\nWarning: Removed 89 rows containing missing values or values outside the scale range\n(`geom_point()`)."
  },
  {
    "objectID": "Geospatial.html",
    "href": "Geospatial.html",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Location Map\n\n\n\n\n\nLand Cover Map\n\n\nLand cover data was obtained from the NLCD a output from the United States Geological Survey\n\n\n\nHistogram of Land Cover"
  },
  {
    "objectID": "Geospatial.html#yellowstone-national-park-and-the-stations-used-in-this-project-land-cover-analysis",
    "href": "Geospatial.html#yellowstone-national-park-and-the-stations-used-in-this-project-land-cover-analysis",
    "title": "Geospatial Analysis",
    "section": "",
    "text": "Location Map\n\n\n\n\n\nLand Cover Map\n\n\nLand cover data was obtained from the NLCD a output from the United States Geological Survey\n\n\n\nHistogram of Land Cover"
  },
  {
    "objectID": "CloudCover.html",
    "href": "CloudCover.html",
    "title": "Cloud Cover and Precipitation",
    "section": "",
    "text": "Load in the data\nPine Creek USGS\nASOS data PNA\nSNOTEL data location: Parker Peak (683)\n\n\nCloud Cover Probability\n\n\n# Add a column that identifys the month\nWest_Yellow_CC &lt;- West_Yellow_CC %&gt;% \n  mutate(month = month(valid, label = TRUE))\n\n# Calculate the cloud frequency \ncloud_frequency &lt;- West_Yellow_CC %&gt;%\n  group_by(month, skyc1) %&gt;%\n  summarize(frequency = n(), .groups = 'drop') %&gt;%\n  complete(month, skyc1, fill = list(frequency = 0)) %&gt;% \n  ungroup()\n\n# Assign values to the differnt conditions\nWest_Yellow_CC &lt;- West_Yellow_CC %&gt;% \n  mutate(cloud_fraction = case_when(\n    skyc1 == \"CLR\" ~ 0,\n    skyc1 == \"FEW\" ~ 0.25,\n    skyc1 == \"SCT\" ~ 0.50,\n    skyc1 == \"OVC\" ~ 0.75,\n    skyc1 == \"BKN\" ~ 1,\n    TRUE ~ NA_real_\n  ))\n\nmonthly_cloud_fraction &lt;- West_Yellow_CC %&gt;%\n  group_by(month) %&gt;%\n  summarize(mean_fraction = mean(cloud_fraction, na.rm = TRUE), .groups = 'drop') %&gt;% \n  ungroup()\n\nmonthly_cloud_fraction &lt;- monthly_cloud_fraction %&gt;% \n  mutate(percentage = mean_fraction * 100)\n\n\n# Calculate the monthly fraction for CC\ncloud_means &lt;- West_Yellow_CC %&gt;% \n  group_by(month) %&gt;% \n  summarise(avg = mean(cloud_fraction))\n\n\n# Step 1: Create a water year and month columns\nWest_Yellow_CC &lt;- West_Yellow_CC %&gt;%\n  mutate(month = month(valid, label = TRUE),  # Extract the month\n         water_year = ifelse(month(valid) &gt;= 10, year(valid) + 1, year(valid)))  # Calculate water year\n\n# Step 2: Group by water year, month, and skyc1, and summarize the frequency\ncloud_frequency &lt;- West_Yellow_CC %&gt;%\n  group_by(month, skyc1) %&gt;%\n  summarize(frequency = n(), .groups = \"drop\") %&gt;% \n  complete(month, skyc1, fill = list(frequency = 0))\n\n# Step 3: View the result\nprint(cloud_frequency)\n\n# A tibble: 84 × 3\n   month skyc1 frequency\n   &lt;ord&gt; &lt;chr&gt;     &lt;int&gt;\n 1 Jan   BKN           0\n 2 Jan   CLR           0\n 3 Jan   FEW           0\n 4 Jan   M             0\n 5 Jan   OVC           0\n 6 Jan   SCT           0\n 7 Jan   VV            0\n 8 Feb   BKN           0\n 9 Feb   CLR           0\n10 Feb   FEW           0\n# ℹ 74 more rows\n\n# Turn the frequency into a percentage\ncloud_frequency &lt;- cloud_frequency %&gt;% \n  group_by(month) %&gt;% \n  mutate(monthly_total = sum(frequency)) \n\ncloud_frequency &lt;- cloud_frequency %&gt;% \n  group_by(month, skyc1) %&gt;% \n  mutate(percentage = frequency/monthly_total * 100) %&gt;% \n  ungroup()\n\ncloud_frequency$month &lt;- factor(cloud_frequency$month, levels = month.abb)\nmonthly_cloud_fraction$month &lt;- factor(monthly_cloud_fraction$month, levels = month.abb)\n\nStacked bar plot\n\nggplot()+\n  geom_col(data = cloud_frequency, aes(x = month, y = percentage, fill = skyc1), position = \"stack\")+\n  labs(\n    x = \"Months\",\n    y = \"Percentage of Cloud Conditions\",\n    fill = \"Cloud Conditions\",\n    caption = \"Figure 1. Cloud conditions percentage for each month at the lowest cloud deck.\"\n  )\n\nWarning: Removed 28 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nMethods for Question 1\nTo determine the cloud cover probability for Yellowstone I first had to find a data set with cloud cover data. The original ASOS station I had didn’t contain cloud cover, but I was able to find West Yellowstone which had data. The first step (1) was to create a column that contained the month name, this was then used to group all the months together. (2) I then split the different categories of cloud cover (brk, clr etc.) into different percentages. (3) After that I was able to determine what the mean cloud cover was throughout the year. (4) The next step was to summarize and count the number of observations and turn that into a percentage of the year by multiplying the number by 100. (5) The final step was to create a stacked bar plot.\nResults for Question 1\nClear was the overall majority of cloud cover conditions throughout the water year year. The majority of those months with a high number of clear days was in the summer. In the early winter months and the early spring the percentage of few clouds and overcase storms increase marginally.\nDiscussion for Question 1\nIn this data set after analysis I discovered that the main winter months for this water year had no data at all. This was not labeled missing becuase it was empty in the original data set and not labeled with a “m”. Besides that the majority of the observations for this year showed that clear was the main cloud conditions. this makes sense when comparing the climate and region with Colorado that has many blue sky days.\n\n\n2 Precipitation Probability\n\n# Create data frame with only precip\nASOS_yellow_precip &lt;- ASOS_yellowstone_original %&gt;% \n  mutate(month = format(valid, \"%Y-%m\"))\n\nASOS_yellow_precip &lt;- ASOS_yellow_precip %&gt;% \n  mutate(precip_event = ifelse(p01i &gt; 0 , 1, 0),\n         non_trace_event = ifelse(p01i &gt;= 0.01, 1, 0))\n\nMonthly_precip &lt;- ASOS_yellow_precip %&gt;% \n  group_by(month) %&gt;% \n  summarise(\n    total_events = n(),\n    precip_fraction = sum(precip_event) / total_events,\n    non_trace_fraction = sum(non_trace_event) / total_events\n  )\n\n\n2 Graph\n\nggplot(Monthly_precip, aes(x = month)) +\n  geom_line(aes(y = precip_fraction, group = 1)) +  \n  geom_line(aes(y = non_trace_fraction, group = 1)) +\n  labs(\n       x = \"Month\",\n       y = \"Precipitation Frequency (Fraction)\",\n       color = \"Event Type\",\n       caption = \"Figure 2. Monthly precipitation frequency at Yellowstone National Park\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1),\n        axis.line = element_line(color = \"black\"),\n        axis.ticks = element_line(color = \"black\"),\n        axis.text = element_text(color = \"black\"))\n\n\n\n\n\n\n\n\nMethods of Question 2\nThe first step (1)  in this analysis was to create a data frame with only precipitation to make the analysis cleaner. (2) The next step was to determine what precipitation events where trace events. This was to remove the trace event so that we can compare them with and without. (3) the next step was to graph the two lines through the year on a line graph.\nResults for Question 2\nAfter plotting the two line on the same plot I was able to determine that there were no trace events less than 0.01 inches. The chance of precipitation in Yellowstone national park mainly occurs in the months of February and March.\nDiscussion for Question 2\nThe data did not contain any trace events. Looking into the original data set there was nothing labeled with a T. The percentage of precipitation events throughout the year mainly occurred in the late winter. This is something that we would expect with the main precipitation mechanism being Orthographic lift. This is caused by the rocky mountains and this is something we expect through the county at this longitude.\n\n\n\n3 Precipitation Mechanism\nIn Yellowstone National Park, precipitation primarily occurs from Orographic lift. Based on research and considering the topographic properties of the park, we find that as moist air rises and moves over the Rocky Mountains, the water contents within the clouds cool and condense. This leads to precipitation on the west or windward side of the Rocky Mountains.\n\n\n4 Compare Frequency using only the lowest cloud deck\n\n# Prepare the data with month and water year columns\nWest_Yellow_CC_all_levels &lt;- West_Yellow_CC %&gt;%\n  mutate(month = month(valid, label = TRUE),\n         water_year = ifelse(month(valid) &gt;= 10, year(valid) + 1, year(valid)))\n\n# Assign cloud fractions\ncloud_conditions &lt;- c(\"CLR\" = 0, \"FEW\" = 0.25, \"SCT\" = 0.5, \"OVC\" = 0.75, \"BKN\" = 1)\nWest_Yellow_CC_all_levels &lt;- West_Yellow_CC_all_levels %&gt;%\n  mutate(cloud_fraction = coalesce(cloud_conditions[skyc1], \n                                    cloud_conditions[skyc2], \n                                    cloud_conditions[skyc3], \n                                    NA_real_))\n\n# Calculate cloud frequency \ncloud_frequency_all_levels &lt;- West_Yellow_CC_all_levels %&gt;%\n  group_by(month, skyc1, skyc2, skyc3) %&gt;%\n  summarise(frequency = n(), .groups = 'drop') %&gt;%\n  complete(month, skyc1, skyc2, skyc3, fill = list(frequency = 0)) %&gt;%\n  group_by(month) %&gt;%\n  mutate(monthly_total = sum(frequency)) %&gt;%\n  mutate(percentage = (frequency / monthly_total) * 100) %&gt;%\n  ungroup()\n\n# Calculate monthly cloud fraction\nmonthly_cloud_fraction &lt;- West_Yellow_CC_all_levels %&gt;%\n  group_by(month, skyc1, skyc2, skyc3) %&gt;%\n  summarise(mean_fraction = mean(cloud_fraction, na.rm = TRUE), .groups = 'drop') %&gt;%\n  mutate(percentage = mean_fraction * 100) %&gt;%\n  ungroup()\n\n# Ensure months are ordered correctly\ncloud_frequency_all_levels$month &lt;- factor(cloud_frequency_all_levels$month, levels = month.abb)\nmonthly_cloud_fraction$month &lt;- factor(monthly_cloud_fraction$month, levels = month.abb)\n\n# Plot\ncloud_frequency_long &lt;- cloud_frequency_all_levels %&gt;%\n  pivot_longer(cols = c(skyc1, skyc2, skyc3), \n               names_to = \"sky_condition\", \n               values_to = \"cloud_condition\") \n\nggplot(data = cloud_frequency_long) +\n  geom_col(aes(x = month, y = percentage, fill = cloud_condition), position = \"stack\") +\n  labs(\n    x = \"Months\",\n    y = \"Percentage of Cloud Conditions\",\n    fill = \"Cloud Conditions\",\n    caption = \" Figure 3. Stacked bar plot showing differnt cloud conditions comapred between differnt layers of clouds.\"\n  ) +\n  facet_wrap(~ sky_condition)+\n  theme(\n    axis.text.x = element_text(angle = 45, hjust = 1, size = 8),\n    strip.text = element_text(size = 12),\n    axis.text = element_text(color = \"black\"),\n    axis.line = element_line(color = \"black\"),\n    axis.ticks = element_line(color = \"black\")\n  )\n\nWarning: Removed 2100 rows containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\n\n\n\n\nMethods for Question 4.\nThe first step (1) that I did to complete this problem was copy the code down from question 1. (2) The next step to complete the analysis was the make a new dataset with all levels. This was done to keep everything clean. (3) Then, I was able to create numeric labels for the different cloud conditions for each cloud cover level. (4) I thin groups all the different months together as well as the sky cover level and calculated the frequency of each type of conditions. (5) I then multiplied this value to get the percentage of time in the month. (6) Finally, I plotted the different stacked bar plots and facet wrapped them by different levels so we could compare the observations.\nResults for Question 4.\nThe general percentage of broken (BKN), overcast (OCV), and scattered (SCT) storms stayed the smae throughout the three levels of cloud cover.\nDiscussion for Question 4.\nWithin the upper two cloud levels (Skyc2 & Skyc3) there is a lot of missing data. There are also no observations of clear sky in the upper conditions. This makes me think that the ASOS station data figures that if there are no cloud and its clear there is nothing for the upper cloud cover conditions to record.\n\n\n5 Precipitation form\n\n# Step 1: Convert 'valid' to a date-time object and extract month\nWest_yellow_2_Q5 &lt;- West_yellow_2 %&gt;%\n  mutate(valid = as.POSIXct(valid, format = \"%Y-%m-%d %H:%M:%S\"),\n         month = month(valid, label = TRUE))\n\n# Convert to numeric and drop na values\nWest_yellow_2_Q5 &lt;- West_yellow_2_Q5 %&gt;%\n  mutate(\n    skyl1 = ifelse(skyl1 == \"M\", NA, skyl1),\n    skyl2 = ifelse(skyl2 == \"M\", NA, skyl2),\n    skyl3 = ifelse(skyl3 == \"M\", NA, skyl3)\n  ) %&gt;%\n  mutate(\n    skyl1 = as.numeric(skyl1),\n    skyl2 = as.numeric(skyl2),\n    skyl3 = as.numeric(skyl3)\n  )\n# Step 2: Assign cloud heights based on sky conditions\nWest_yellow_2_Q5 &lt;- West_yellow_2_Q5 %&gt;%\n  mutate(\n    cloud_height_l1 = case_when(\n      skyc1 == \"CLR\" ~ 0,\n      skyc1 %in% c(\"FEW\", \"SCT\", \"BKN\", \"OVC\") ~ skyl1 * 0.3048,\n      TRUE ~ NA_real_\n    ),\n    cloud_height_l2 = case_when(\n      skyc2 == \"CLR\" ~ 0,\n      skyc2 %in% c(\"FEW\", \"SCT\", \"BKN\", \"OVC\") ~ skyl2 * 0.3048,\n      TRUE ~ NA_real_\n    ),\n    cloud_height_l3 = case_when(\n      skyc3 == \"CLR\" ~ 0,\n      skyc3 %in% c(\"FEW\", \"SCT\", \"BKN\", \"OVC\") ~ skyl3 * 0.3048,\n      TRUE ~ NA_real_\n    )\n  )\n\n# Step 3: Define average monthly temperatures (°C) - adjust as necessary\navg_monthly_temp &lt;- West_yellow_2_Q5 %&gt;% \n  mutate(tmpf = na_if(tmpf, \"M\"),\n         tmpf = as.numeric(tmpf)) %&gt;% \n  group_by(month) %&gt;% \n  summarise(avg_temp = mean(tmpf, na.rm = TRUE), .groups = 'drop')\n\n# Step 4: Combine height and temperature data\nprecip_distribution &lt;- West_yellow_2_Q5 %&gt;%\n  left_join(avg_monthly_temp, by = \"month\") %&gt;%\n  mutate(\n    # Calculate temperature at cloud heights using a lapse rate of 6.5 °C per 1000 m\n    temperature_at_height_l1 = avg_temp - (5 * (cloud_height_l1 / 1000)),\n    temperature_at_height_l2 = avg_temp - (5 * (cloud_height_l2 / 1000)),\n    temperature_at_height_l3 = avg_temp - (5 * (cloud_height_l3 / 1000)),\n    \n    # Determine precipitation types\n    precipitation_type_l1 = case_when(\n      temperature_at_height_l1 &lt;= 32 ~ \"Solid (Snow)\",\n      temperature_at_height_l1 &gt; 32 ~ \"Liquid (Rain)\",\n      TRUE ~ \"No Precipitation\"\n    ),\n    precipitation_type_l2 = case_when(\n      temperature_at_height_l2 &lt;= 32 ~ \"Solid (Snow)\",\n      temperature_at_height_l2 &gt; 32 ~ \"Liquid (Rain)\",\n      TRUE ~ \"No Precipitation\"\n    ),\n    precipitation_type_l3 = case_when(\n      temperature_at_height_l3 &lt;= 32 ~ \"Solid (Snow)\",\n      temperature_at_height_l3 &gt; 32 ~ \"Liquid (Rain)\",\n      TRUE ~ \"No Precipitation\"\n    )\n  )\n\n# Step 5: Summarize the monthly distribution of precipitation types\nmonthly_distribution &lt;- precip_distribution %&gt;%\n  group_by(month) %&gt;%\n  summarise(\n    solid_count_l1 = sum(precipitation_type_l1 == \"Solid (Snow)\", na.rm = TRUE),\n    liquid_count_l1 = sum(precipitation_type_l1 == \"Liquid (Rain)\", na.rm = TRUE),\n    solid_count_l2 = sum(precipitation_type_l2 == \"Solid (Snow)\", na.rm = TRUE),\n    liquid_count_l2 = sum(precipitation_type_l2 == \"Liquid (Rain)\", na.rm = TRUE),\n    solid_count_l3 = sum(precipitation_type_l3 == \"Solid (Snow)\", na.rm = TRUE),\n    liquid_count_l3 = sum(precipitation_type_l3 == \"Liquid (Rain)\", na.rm = TRUE),\n    .groups = 'drop'\n  )\n\n# View the monthly distribution\nprint(monthly_distribution)\n\n# A tibble: 8 × 7\n  month solid_count_l1 liquid_count_l1 solid_count_l2 liquid_count_l2\n  &lt;ord&gt;          &lt;int&gt;           &lt;int&gt;          &lt;int&gt;           &lt;int&gt;\n1 Apr                0               0              0               0\n2 May              167             506            117             176\n3 Jun                0             719              0             141\n4 Jul                0             743              0             108\n5 Aug                0             716              0             101\n6 Sep               50             624             15             133\n7 Oct              186             337            143              25\n8 Nov                0               0              0               0\n# ℹ 2 more variables: solid_count_l3 &lt;int&gt;, liquid_count_l3 &lt;int&gt;\n\n# Plot the table\nMonthly_distribution_long &lt;- monthly_distribution %&gt;%\n  pivot_longer(cols = -month, names_to = c(\"type\", \"layer\"), names_sep = \"_\", values_to = \"count\")\n\nWarning: Expected 2 pieces. Additional pieces discarded in 6 rows [1, 2, 3, 4,\n5, 6].\n\n# Step 2: Create the plot\nggplot(Monthly_distribution_long, aes(x = month, y = count, fill = type)) +\n  geom_col(position = \"stack\") +\n  facet_wrap(~ layer, ncol = 1) +  # Create separate plots for each layer\n  labs(\n    x = \"Month\",\n    y = \"Count\",\n    fill = \"Precipitation Type\",\n    caption = \" Figure 4. Total number of precipitation events in the year broken up into solid precip versus liquid precip.\"\n  )  +\n  scale_fill_manual(values = c(\"solid\" = \"blue\", \"liquid\" = \"lightblue\"))+\n  theme(\n    axis.line = element_line(color = \"black\"),\n    axis.text = element_text(color =\"black\"),\n    axis.ticks = element_line(color =\"black\")\n  )\n\n\n\n\n\n\n\n\nMethods for Question 5.\nTo begin analysis (1) on this problem I first began with creating a new data frame for this problem. I then began with creating a column to specify the month. The next step (2) was to drop the na value in the sky cover columns. (3) Then I converted the elevation column to kilometers so that I could put it into the lapse rate equation. (4) I then found the average monthly temperature to put into the lapse rate equation. (5) I then created a model to determine if it was above freezing it would be liquid and below would be solid. (6) I was then able to count all of the observations and determine the number of events each year that are solid precipitation versus liquid."
  },
  {
    "objectID": "StationSummary.html",
    "href": "StationSummary.html",
    "title": "Site Selection and Station Summary",
    "section": "",
    "text": "Parameter\nValue\n\n\n\n\nLocation of Station (state)\nWyoming\n\n\nMeteorological Station Name\nRAMOS as Yellowstone\n\n\nMeteorological Station Code\n72034594086\n\n\nMeteorological Station Latitude\n42.795556\n\n\nMeteorological Station Longitude\n-109.807222\n\n\nMeteorological Station Elevation\n2713 meters\n\n\nSNOTEL Station Name\nParker Peak\n\n\nSNOTEL Station Number\n643\n\n\nSNOTEL Station Latitude\n43.01\n\n\nSNOTEL Station Longitude\n-109.76\n\n\nSNOTEL Station Elevation\n2865.12 meters\n\n\nHydrometric Station Name\nPine Creek Above Fremont Lake\n\n\nHydrometric Station Number\n09196500\n\n\nBasin Area\n197.6161 km²\n\n\nHydrometric Station Latitude\n43.027056\n\n\nHydrometric Station Longitude\n-109.773528\n\n\nHydrometric Station Elevation\n2270.76 meters\n\n\nPrecip. Gauge Type\nCannot Find this Info\n\n\nPrecip. Gauge Shield\nCannot Find this Info"
  },
  {
    "objectID": "StationSummary.html#site-selection-station-summary-assignment-1",
    "href": "StationSummary.html#site-selection-station-summary-assignment-1",
    "title": "Site Selection and Station Summary",
    "section": "",
    "text": "Parameter\nValue\n\n\n\n\nLocation of Station (state)\nWyoming\n\n\nMeteorological Station Name\nRAMOS as Yellowstone\n\n\nMeteorological Station Code\n72034594086\n\n\nMeteorological Station Latitude\n42.795556\n\n\nMeteorological Station Longitude\n-109.807222\n\n\nMeteorological Station Elevation\n2713 meters\n\n\nSNOTEL Station Name\nParker Peak\n\n\nSNOTEL Station Number\n643\n\n\nSNOTEL Station Latitude\n43.01\n\n\nSNOTEL Station Longitude\n-109.76\n\n\nSNOTEL Station Elevation\n2865.12 meters\n\n\nHydrometric Station Name\nPine Creek Above Fremont Lake\n\n\nHydrometric Station Number\n09196500\n\n\nBasin Area\n197.6161 km²\n\n\nHydrometric Station Latitude\n43.027056\n\n\nHydrometric Station Longitude\n-109.773528\n\n\nHydrometric Station Elevation\n2270.76 meters\n\n\nPrecip. Gauge Type\nCannot Find this Info\n\n\nPrecip. Gauge Shield\nCannot Find this Info"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "William ‘Billy’ Johnson",
    "section": "",
    "text": "Welcome to my website! My name is William “Billy” Johnson, and I’m a graduate student at Colorado State University pursuing a Professional Master of Science in Water Resources. I created this site as a place to share and document my work in hydrology, watershed science, and cold water fisheries.\nMy academic and professional interests focus on understanding and solving complex water challenges in the Western United States. From snowpack dynamics and streamflow modeling to remote sensing and river restoration, I enjoy working at the intersection of data science and field-based environmental research.\nThis site showcases a range of projects through both coursework and independent research. Whether you’re interested in water resource management, aquatic ecology, or simply curious about how data and ecosystems connect, I invite you to explore and reach out.\n\n\n\nWilliam’s Image"
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "William ‘Billy’ Johnson",
    "section": "",
    "text": "Welcome to my website! My name is William “Billy” Johnson, and I’m a graduate student at Colorado State University pursuing a Professional Master of Science in Water Resources. I created this site as a place to share and document my work in hydrology, watershed science, and cold water fisheries.\nMy academic and professional interests focus on understanding and solving complex water challenges in the Western United States. From snowpack dynamics and streamflow modeling to remote sensing and river restoration, I enjoy working at the intersection of data science and field-based environmental research.\nThis site showcases a range of projects through both coursework and independent research. Whether you’re interested in water resource management, aquatic ecology, or simply curious about how data and ecosystems connect, I invite you to explore and reach out.\n\n\n\nWilliam’s Image"
  },
  {
    "objectID": "metemorphism.html",
    "href": "metemorphism.html",
    "title": "Densification & Metamorphism",
    "section": "",
    "text": "Billy Johnson Dr Fassnacht October 31, 2024 WR 574 Snow Hydrology\nAssignment 8. Densification and Metamorphism\nMethods\nWithin that stored dataset I created a loop to iterate through each observation in the results. With this I was able to create a lag column with the density and SWE that combined the previous time steps. I set the max density at 300 kg/m3 & set kd = 0.01 as a coefficient, using Verseghy’s 1991 first order decay function (equation 2). With the density of old snow and the old snow SWE from the previous time step I was able to calculate the old snow depth to move on to the final part. The final if function that I created to calculate the snowpack metamorphism I stated if temperatures are below 0°C to stop rain from being added I kept all observations of old snow. With old snow SWE and depth because we can add those together to sum up the whole year, I was able to calculate the combined density (figure 2). The final part was getting fresh snowfall from a previous assignment and combine that with the total snow depth with metamorphism (figure 1.)\nTo complete the analysis comparing the estimated bulk density from the SNOTEL data with a computed density from Sturm et al 2010 (equation 3). I used given values according to an alpine environment to fill in rho max and rho original. Using those given value, I was able to calculate the computed density from Sturm et al 2010. I finally combined the two time series plots (figure 3.)\nResults\nAfter computing the total snow depth with metamorphism, we can see that the peak of snow depth at Parker Peak in Yellowstone National Park was about 1,000 mm. This station is located at 9400 feet in elevation. The fresh snow throughout the year cumulated to give use a total of 4,000 mm in snowfall throughout the year (figure 1).\nUsing the computed total depth with metamorphism I was able to calculate bulk density. The density max wax a set 300 kg/m3. Figure 2 shows the snowpack until it is melted in late June.\nThe final part of this analysis was to compare the computed density to the estimated bulk density. We see that the Sturm et al 2010 computed density is much higher at the end of the year than our estimated bulk density. The estimated bulk density has a limit on the max rho as well.\n![Figure 3. Time series plot showing the bulk density over time compared with the calculated density using equation from Strum et al 2010] (Images/Metemorphism/figure3.jpg)\nDiscussion\nEquations 1. s = 67.92 + 51.25 e(Ts/2.59) 2. s(t) = [s(t-1) - s-max] e -kdt + s-max 3. s = (s-max - s-0) [ 1 – e-k1 x ds – k2 X DOY] + s-0"
  },
  {
    "objectID": "metemorphism.html#snowpack-metamorphism-using-asos-hourly-data",
    "href": "metemorphism.html#snowpack-metamorphism-using-asos-hourly-data",
    "title": "Densification & Metamorphism",
    "section": "1. Snowpack Metamorphism (using ASOS hourly data):",
    "text": "1. Snowpack Metamorphism (using ASOS hourly data):\n\nAssume a bulk snowpack, i.e., only new (fresh) and old snow. -Use the first order exponential function to model the densification of a snowpack. -Chose an appropriate maximum snowpack density by considering the maximum density at peak SWE from the SNOTEL station or the Pomeroy and Gray [1995] formulation. Note that the rate of densification can also be altered. -Allow the new snow to fall at its fresh snow density and determine the net snowpack depth as a function of the old snow and new snow. -On one plot show the snow depth with metamorphism that you computed in the question and the hourly fresh snow depth or snowfall from the previous assignment. (This is analogous to SNOTEL question in the previous assignment). The units for depth are meters. -On a second plot show the hourly bulk snow pack density (in kg/m3)."
  },
  {
    "objectID": "metemorphism.html#snotel-metamorphism-modeling-compare-the-estimated-bulk-snowpack-density-from-your-snotel-data-previous-assignment-to-a-computed-density-using-a-formulation-from-mizukami-and-perica-2008-sturm-et-al.-2010-or-sexstone-and-fassnacht-2014.",
    "href": "metemorphism.html#snotel-metamorphism-modeling-compare-the-estimated-bulk-snowpack-density-from-your-snotel-data-previous-assignment-to-a-computed-density-using-a-formulation-from-mizukami-and-perica-2008-sturm-et-al.-2010-or-sexstone-and-fassnacht-2014.",
    "title": "Densification & Metamorphism",
    "section": "2. SNOTEL Metamorphism Modeling: Compare the estimated bulk snowpack density from your SNOTEL data (previous assignment) to a computed density using a formulation from Mizukami and Perica (2008), Sturm et al. (2010), or Sexstone and Fassnacht (2014).",
    "text": "2. SNOTEL Metamorphism Modeling: Compare the estimated bulk snowpack density from your SNOTEL data (previous assignment) to a computed density using a formulation from Mizukami and Perica (2008), Sturm et al. (2010), or Sexstone and Fassnacht (2014).\n\npsmax &lt;- 597.5\nps0 &lt;- 223.7\nk1 &lt;- 0.0012\nk2 &lt;- 0.0038\n\nASOS_snow_calcs &lt;- ASOS_snow_calcs %&gt;% \n  mutate(day_of_year = yday(valid)) %&gt;% \n  mutate(day_of_year = ifelse(day_of_year &gt; 273, day_of_year - 365, day_of_year))\n\n# Strum\nASOS_snow_calcs &lt;- ASOS_snow_calcs %&gt;% \n  mutate(strum_density = (psmax - ps0)*(1 - exp(-k1 * total_snow_depth - k2 * day_of_year)) + ps0)\n\nggplot()+\n  geom_line(data = ASOS_snow_calcs, aes(x = valid, y = combined_density, color = \"Bulk Density\"))+\n  geom_line(data = ASOS_snow_calcs, aes(x = valid, y = strum_density, color = \"Calculated Density (Strum et al 2010\"))+\n  labs(\n    color = \"Snow Density Type\",\n    y = \"Density (kg/m3)\",\n    x = \"Date\"\n  )+\n  theme_linedraw()+\n  theme(\n  legend.position = c(0.75,0.20),\n  legend.background = element_rect(fill = \"white\", color = \"black\")\n)"
  },
  {
    "objectID": "Canopy.html",
    "href": "Canopy.html",
    "title": "Canopy Interception",
    "section": "",
    "text": "Billy Johnson Dr Fassnacht October 16, 2024 WR 574 Snow Hydrology\nCanopy Interception\nCanopy Interception Observations:\nWhen going outside here in Northern Colorado the deciduous tree that occurs in big quantities in Fort Collins CSU campus was the Bur Oak (Quercus macrocarpa). This tree has big leaves creating a big leaf area index for a smaller tree. The branches are big a strong and do not have a lot of flexibility. When rain falls on the canopy of these Oak trees there can be a big difference in the amount of rain interception based on the time of year. As these trees lose their leaves in the fall and don’t grow them again until the summer there are periods on both sides that may have rain events that do not intercept rain at all. When the Oak tree is in full bloom the big broad leaves on the tree will hold more water. There can be more evaporation from the exposed water droplets. With the broad leaf trees there is more potential for transpiration and the tree may use those broad leaves to capture water for growth. When snowfall interacts with these deciduous trees generally there isn’t much interception. If we have early or late storm events these trees can hold a lot more snow on their leaves and this can cause limbs to break as the branches are not flexible enough to shed snow.\nIn contrast Colorado is also home to various species of evergreen trees. The Blue Spruce (Picea pungens) is an evergreen tree that contains single needles. The branches on these trees are big and flexible and most are positioned with a downward trajectory. The canopy from this tree goes all the way down to the ground. When rainfalls on these trees there isn’t much rainfall retention. The rain droplets will shed down the needles and will drip down the tree. The canopy is much denser in these trees but with the shape of these needles they can shed water. This also contradicts when the snow begins to fall. As these trees do not lose their leaves during the winter months when big snow events come the tree will capture much more snow. The big flexible branches allow the tree to shed snow when the weight is too much. In relation to the Blue Spruce another evergreen tree that is found in Colorado is the Rocky Mountain Juniper tree (Juniperus scopulorum). This is another tree with needle like foliage. These needles are much thinner and smaller and are not as strong as the Blue Spruce. Unlike the Blue Spruce most of the canopy for these trees is located towards the top of the tree. With the characteristics of the foliage on these trees we can infer that they are not good at intercepting rainfall. The leaves are not big enough to capture and hold rainfall. These trees will shed rainfall throughout the year. In comparison to the Blue Spruce when snow begins to fall in Colorado the Juniper tree will not lose its foliage. This will allow for the tree to capture more snowfall than rain. This tree on the other hand is very tall and thin and will not capture as much snow as the blue spruce. The branches are not directed downwards, and it doesn’t appear to be built like it would shed heavy amounts of snow but because of the characteristic the tree wouldn’t capture enough snow to break limbs but does capture more snow than a common deciduous tree.\n\nCanopy Interception Modeling:\n\n• Assume one vegetation type for the vegetated portion of your watershed (% vegetated estimated in the second assignment). o Shrub & evergreen ~ 90%\n• With a constant leaf area index for the winter and another for the summer (assume June 1st), determine the amount of interception during each storm event, using T=0oC as the threshold between rain and snow. o Winter ~ LAI = 0.5 o Summer ~ LAI = 4\nTo begin analysis on this problem the first thing that I had to do was define the constant variables. I used table 7-1 (Scurlock et al, 2001) that had approximate LAI for different species of trees. Because my watershed contained 53% shrubs and 37% evergreen I used the seasonal shrubs LAI for this problem. The max LAI was 4 and the min was 0.5. I claimed that the fraction of area covered was 90%. I used the threshold of 0°C for determining rain vs snow. The entire area of my watershed is 80 km2. After I defined all my constants, I was able to being data cleaning. I first labeled each observation as summer or winter, and then defined the leaf area index to winter or summer as well. I then went through and filled a IF statement that asked if it was colder than 0°C then label it as rain or snow accordingly. After this I applied a common interception equation (P_mm x LAI x fraction_veg). Finally, I averaged the monthly interception values and cleaned the results in table 1.\nTable 1. Table shows the interception values for shrub and evergreen vegetation at Yellowstone National Park for water year 2023. Month Total Interception (mm) Average Monthly Interception (mm) Average Interception Per Storm (mm) Jan 1.14E-01 1.57E-04 1.63E-03 Feb 0 0 0 Mar 3.43E-01 4.62E-04 2.29E-03 Apr 5.72E-01 1.05E-03 1.14E-02 May 3.43E+00 5.68E-03 4.40E-02 Jun 5.49E+00 7.97E-03 2.61E-01 Jul 0 0 0 Aug 2.74E+00 6.52E-03 1.61E-01 Sep 1.37E+00 2.11E-03 2.98E-02 Oct 0 0 0 Nov 4.57E-01 6.73E-04 8.63E-03 Dec 0 0 0\nResults: Through this analysis we found that June had the highest average monthly interception at 7.97E-03 mm. There are a considerable number of months that didn’t record any interception. All the values calculated throughout the year are relatively low. August and November both produced interception values that resulted very close, around 6.5E-03 (table 1). \n\n\n\nDiscussion: With these results in mind, having the highest month of interception be June makes some sense. With the increased amount of foliage in the summer and with most of the vegetation cover being shrubs then those plants will be in full bloom at this point. With previous assignment we know there was a recorded snow event in June. This could be the cause of the higher interception values with snow falling on vegetation with max leaf area index. When going through the process of calculating the interception these values showed up very low. Review of analysis will be needed to further quantify these results. There are many results that developed 0 for the interception. I can infer that this is not correct because those months will have had precipitation events. July resulted in a zero, but this is a month that would have high vegetation and a high LAI suggesting that there would be more interception than 0. \n\nLoad in the packages\n\nsource(\"Setup_WR574 copy.R\")\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.4     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\nLinking to GEOS 3.10.2, GDAL 3.4.2, PROJ 8.2.1; sf_use_s2() is TRUE\n\nterra 1.7.65\n\n\nAttaching package: 'terra'\n\n\nThe following object is masked from 'package:tidyr':\n\n    extract\n\n\nTo enable caching of data, set `options(tigris_use_cache = TRUE)`\nin your R script or .Rprofile.\n\n\nAttaching package: 'tigris'\n\n\nThe following object is masked from 'package:terra':\n\n    blocks\n\n\nelevatr v0.99.0 NOTE: Version 0.99.0 of 'elevatr' uses 'sf' and 'terra'.  Use \nof the 'sp', 'raster', and underlying 'rgdal' packages by 'elevatr' is being \ndeprecated; however, get_elev_raster continues to return a RasterLayer.  This \nwill be dropped in future versions, so please plan accordingly.\n\n\n\n\nLoad in the data\nPine Creek USGS\n\n# Soda Butte Cr at Park Boundary at Silver Gate (06187915)\nsitenumber &lt;- \"06187915\"\nparametercd &lt;- c(\"00060\", \"00010\")\nstartdate &lt;- \"2023-09-01\"\nenddate &lt;- \"2024-08-31\"\n\nrawUSGSdata &lt;- readNWISdv(sitenumber, parametercd, startdate, enddate)\n\nGET:https://waterservices.usgs.gov/nwis/dv/?site=06187915&format=waterml%2C1.1&ParameterCd=00060%2C00010&StatCd=00003&startDT=2023-09-01&endDT=2024-08-31\n\n\nASOS data PNA\n\n# Station number P60\nASOS_yellowstone_original &lt;- read_csv(\"DataIn/P60.csv\")\n\nWarning: One or more parsing issues, call `problems()` on your data frame for details,\ne.g.:\n  dat &lt;- vroom(...)\n  problems(dat)\n\n\nRows: 7370 Columns: 33\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (24): station, drct, sknt, mslp, vsby, gust, skyc1, skyc2, skyc3, skyc4...\ndbl   (8): lon, lat, elevation, tmpf, dwpf, relh, p01i, alti\ndttm  (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nASOS_CC &lt;- read_csv(\"DataIn/CloudCover.csv\")\n\nRows: 7370 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): station, skyc1, skyc2, skyc3, skyl1, skyl2, skyl3, wxcodes\ndbl  (3): lon, lat, elevation\ndttm (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n# ASOS station in Montana. Station = West Yellowstone (WYS)\nWest_Yellow_Cloud &lt;- read_csv(\"DataIn/WestYellowstone_CloudCover.csv\")\n\nRows: 4488 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (8): station, skyc1, skyc2, skyc3, skyl1, skyl2, skyl3, wxcodes\ndbl  (3): lon, lat, elevation\ndttm (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nWest_yellow_original &lt;- read_csv(\"DataIn/WYS.csv\")\n\nRows: 4488 Columns: 30\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (28): station, tmpf, dwpf, relh, drct, sknt, alti, mslp, vsby, gust, sk...\ndbl   (1): p01i\ndttm  (1): valid\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nSNOTEL data location: Parker Peak (683)\n\nSNOTEL_Parker &lt;- read_csv(\"DataIn/683_STAND_WATERYEAR=2024_clean.csv\")\n\nRows: 341 Columns: 10\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (1): Date\ndbl (8): Site Id, WTEQ.I-1 (in), PREC.I-1 (in), TOBS.I-1 (degC), TMAX.D-1 (d...\nlgl (1): Time\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\n\n\n\nWorkflow\n\n# Clean the data and save the new dataframe\nASOS_accumulation &lt;- ASOS_yellowstone_original %&gt;% \n  select(c(valid, tmpf, p01i, relh, sknt)) %&gt;% \n  mutate(tmp_c = (5/9 * (tmpf - 32))) %&gt;% \n  mutate(Td = tmp_c - ((100 - relh)/5)) %&gt;% \n  mutate(P_mm = p01i * 25.4)\n  \nASOS_accumulation_clean &lt;- ASOS_accumulation %&gt;% \n  filter(tmp_c &lt; 0 & Td &lt; 0) %&gt;% \n  filter(p01i &gt; 0) %&gt;% \n  filter(sknt &lt; 10)\n\n# Add a column that calculates the cumulative sum\nASOS_accumulation_clean &lt;- ASOS_accumulation_clean %&gt;% \n  mutate(P_mm = p01i * 24.5) %&gt;% \n  mutate(cumm_prec = cumsum(P_mm)) \n\n\nwrite_csv(ASOS_accumulation_clean, file = \"DataOut/ASOS_clean.csv\")\n\n\n# Define constants\nLAI_winter &lt;- 0.5\nLAI_summer &lt;- 4\nf_veg &lt;- 0.9  \nthreshold_temp &lt;- 0  \narea_km2 &lt;- 80.03063\narea_veg_km2 &lt;- area_km2 * f_veg  \n\n# Calculate interception\nASOS_accumulation_2 &lt;- ASOS_accumulation %&gt;%\n  mutate(\n    season = if_else(month(valid) == 6, \"summer\", \"winter\"),  \n    LAI = if_else(season == \"summer\", LAI_summer, LAI_winter),  \n    precipitation_type = if_else(tmp_c &gt;= threshold_temp, \"rain\", \"snow\"),  \n    interception = case_when(\n      precipitation_type == \"rain\" ~ P_mm * LAI * f_veg,\n      precipitation_type == \"snow\" ~ P_mm * LAI * f_veg,\n      TRUE ~ 0\n    ),\n    # Set interception to zero 6 hours after the storm\n    interception = if_else(valid %in% (valid + hours(6)), 0, interception)\n  )\n\n# Step 4: Calculate monthly averages\nMonthly_Interception &lt;- ASOS_accumulation_2 %&gt;%\n  group_by(month = month(valid, label = TRUE), year = year(valid)) %&gt;%\n  summarize(\n    total_interception_all = sum(interception, na.rm = TRUE),\n    total_hours = n(),\n    avg_interception_all = total_interception_all / total_hours,\n    total_interception_storm = sum(interception[P_mm &gt; 0], na.rm = TRUE), \n    storm_hours = sum(P_mm &gt; 0),  \n    avg_interception_storm = if_else(storm_hours &gt; 0, total_interception_storm / storm_hours, 0) \n  )\n\n`summarise()` has grouped output by 'month'. You can override using the\n`.groups` argument.\n\nwrite_csv(Monthly_Interception, file = \"DataOut/Monthly_Interception.csv\")\ntibble(Monthly_Interception)\n\n# A tibble: 12 × 8\n   month  year total_interception_all total_hours avg_interception_all\n   &lt;ord&gt; &lt;dbl&gt;                  &lt;dbl&gt;       &lt;int&gt;                &lt;dbl&gt;\n 1 Jan    2024                  0.114         727             0.000157\n 2 Feb    2024                  0             692             0       \n 3 Mar    2024                  0.343         742             0.000462\n 4 Apr    2024                  0.572         546             0.00105 \n 5 May    2024                  3.43          604             0.00568 \n 6 Jun    2024                  5.49          688             0.00797 \n 7 Jul    2024                  0             140             0       \n 8 Aug    2024                  2.74          421             0.00652 \n 9 Sep    2023                  1.37          651             0.00211 \n10 Oct    2023                  0             736             0       \n11 Nov    2023                  0.457         679             0.000673\n12 Dec    2023                  0             744             0       \n# ℹ 3 more variables: total_interception_storm &lt;dbl&gt;, storm_hours &lt;int&gt;,\n#   avg_interception_storm &lt;dbl&gt;"
  }
]